# Nomad Server Configuration Template
# Purpose: Configures Nomad server nodes that manage job scheduling, resource allocation,
#          and orchestration across the ProposalsApp infrastructure.
#
# Architecture Context:
# - 3-server cluster spanning all datacenters (dc1, dc2, dc3)
# - Unlike Consul, Nomad servers form a single global cluster
# - All servers participate in Raft consensus for job state
# - Workloads can be scheduled to any datacenter based on constraints
#
# Where this runs:
# - consul-nomad-xxx nodes (control plane containers)
# - Co-located with Consul servers for reduced latency

# Datacenter and Region Configuration
# datacenter: Local datacenter identifier (dc1, dc2, dc3)
# region: Global region for all datacenters (used for multi-region federation)
datacenter = "{{ datacenter }}"
region = "{{ nomad_region }}"

# Data persistence directory
# Stores job definitions, allocation state, and Raft data
data_dir = "/var/lib/nomad"

# Logging configuration
log_level = "INFO"

# Network binding
# bind_addr: Listen on all interfaces (0.0.0.0)
# This allows both local and Tailscale connections
bind_addr = "0.0.0.0"

# Advertisement configuration
# These IPs are what other Nomad nodes use to connect to this server
# Using Tailscale IPs ensures encrypted communication between datacenters
advertise {
  http = "{{ tailscale_ip | default(ansible_default_ipv4.address) }}"  # HTTP API (port 4646)
  rpc = "{{ tailscale_ip | default(ansible_default_ipv4.address) }}"   # Internal RPC (port 4647)
  serf = "{{ tailscale_ip | default(ansible_default_ipv4.address) }}"  # Gossip protocol (port 4648)
}

# Server Configuration
server {
  enabled = true
  
  # Raft consensus configuration
  # bootstrap_expect = 3 means wait for exactly 3 servers before electing a leader
  # This prevents split-brain scenarios during initial cluster formation
  bootstrap_expect = 3
  
  # Gossip encryption key
  # Protects server-to-server communication from tampering
  # Must be identical across all Nomad servers (generated by nomad operator keygen)
  encrypt = "{{ nomad_encrypt_key }}"
  
  # Multi-region federation
  # authoritative_region: Primary region for ACL tokens and policies
  # In single-region deployments, this is the only region
  authoritative_region = "{{ nomad_region }}"
  
  # Scheduler Configuration
  # Controls how Nomad places workloads across the infrastructure
  default_scheduler_config {
    # Scheduling algorithm:
    # - "spread": Distributes allocations evenly across nodes (better for HA)
    # - "binpack": Packs allocations tightly (better for resource efficiency)
    scheduler_algorithm = "spread"
    
    # Preemption Configuration
    # Allows high-priority jobs to evict lower-priority ones when resources are scarce
    preemption_config {
      system_scheduler_enabled = true   # System jobs can preempt others
      service_scheduler_enabled = true  # Service jobs can preempt lower priority
      batch_scheduler_enabled = true    # Batch jobs can preempt lower priority
    }
  }
  
  # Server discovery and clustering
  # Automatically retry joining these servers until successful
  # This enables automatic cluster formation after restarts
  server_join {
    retry_join = [
      {% for host in groups['nomad_servers'] %}
      {% if host != inventory_hostname %}
      "{{ hostvars[host].tailscale_ip | default(hostvars[host].ansible_default_ipv4.address) }}",
      {% endif %}
      {% endfor %}
    ]
  }
  
  # Heartbeat configuration
  # Controls how quickly Nomad detects node failures
  heartbeat_grace = "60s"  # Reduced from default 5m to 1m for faster failure detection
}

# Client Configuration
# Disabled on server nodes - they only manage, don't run workloads
client {
  enabled = false
}

# Consul Integration
# Enables automatic service registration and health checking
consul {
  # Connect to local Consul agent
  address = "127.0.0.1:{{ consul_client_port }}"
  
  # Service names for registration in Consul
  server_service_name = "nomad"         # How servers appear in Consul
  client_service_name = "nomad-client"  # How clients appear in Consul
  
  # Automatic registration features
  auto_advertise = true      # Register Nomad services in Consul
  server_auto_join = true    # Use Consul to discover other servers
  client_auto_join = true    # Use Consul to discover clients
}

# Access Control Lists
# Currently disabled for operational simplicity
# Enable in production with proper token management
acl {
  enabled = false
}

# Telemetry Configuration
# Exposes metrics for Prometheus monitoring
telemetry {
  collection_interval = "1s"         # How often to collect metrics
  disable_hostname = true            # Cleaner metric names
  prometheus_metrics = true          # Enable Prometheus format
  publish_allocation_metrics = true  # Job/task metrics
  publish_node_metrics = true        # Server health metrics
}

# Task Driver Plugins
# These define what types of workloads Nomad can run

# Raw Exec Plugin
# Disabled for security - would allow arbitrary command execution
plugin "raw_exec" {
  config {
    enabled = false
  }
}

# Docker Plugin
# Primary task driver for containerized workloads
plugin "docker" {
  config {
    # Docker daemon connection
    endpoint = "unix:///var/run/docker.sock"
    
    # Garbage collection settings
    # Automatically clean up stopped containers and unused images
    gc {
      image = true          # GC unused images
      image_delay = "3m"    # Keep images for 3 minutes after last use
      container = true      # GC stopped containers
    }
    
    # Volume management
    volumes {
      enabled = true        # Allow jobs to mount volumes
    }
    
    # Security settings
    allow_privileged = false  # Prevent privileged containers
  }
}