#!/bin/bash
# GPU Health Check Script for LXC Containers
# This script monitors GPU availability and health

set -euo pipefail

# Configuration
CONTAINER_NAME="{{ inventory_hostname }}"
LOG_FILE="/var/log/gpu-health-check.log"
CONSUL_SERVICE="gpu-{{ inventory_hostname }}"

# Logging function
log() {
    echo "[$(date +'%Y-%m-%d %H:%M:%S')] $1" | tee -a "$LOG_FILE"
}

# Check if this container should have GPU access
{% if inventory_hostname in gpu_passthrough_containers %}
HAS_GPU=true
{% else %}
HAS_GPU=false
{% endif %}

if [ "$HAS_GPU" = "false" ]; then
    log "INFO: This container is not configured for GPU access"
    exit 0
fi

# Function to register health check result with Consul
register_consul_health() {
    local status=$1
    local notes=$2
    
    if command -v consul &>/dev/null; then
        if [ "$status" = "passing" ]; then
            consul kv put "health/gpu/${CONTAINER_NAME}/status" "healthy" >/dev/null 2>&1 || true
            consul kv put "health/gpu/${CONTAINER_NAME}/last_check" "$(date -u +%Y-%m-%dT%H:%M:%SZ)" >/dev/null 2>&1 || true
            consul kv put "health/gpu/${CONTAINER_NAME}/notes" "$notes" >/dev/null 2>&1 || true
        else
            consul kv put "health/gpu/${CONTAINER_NAME}/status" "unhealthy" >/dev/null 2>&1 || true
            consul kv put "health/gpu/${CONTAINER_NAME}/last_check" "$(date -u +%Y-%m-%dT%H:%M:%SZ)" >/dev/null 2>&1 || true
            consul kv put "health/gpu/${CONTAINER_NAME}/notes" "$notes" >/dev/null 2>&1 || true
        fi
    fi
}

# Initialize
log "Starting GPU health check for $CONTAINER_NAME"
ERRORS=0
STATUS_NOTES=""

# Check 1: NVIDIA device files
log "Checking NVIDIA device files..."

# Check control devices (always required)
CONTROL_DEVICES=(
    "/dev/nvidiactl"
    "/dev/nvidia-uvm"
)

for device in "${CONTROL_DEVICES[@]}"; do
    if [ ! -e "$device" ]; then
        log "ERROR: Missing control device: $device"
        ERRORS=$((ERRORS + 1))
        STATUS_NOTES="${STATUS_NOTES}Missing device: $device; "
    fi
done

# Check GPU devices (dynamic detection)
GPU_COUNT=0
for device in /dev/nvidia[0-9]*; do
    if [ -e "$device" ]; then
        log "Found GPU device: $device"
        GPU_COUNT=$((GPU_COUNT + 1))
    fi
done

if [ $GPU_COUNT -eq 0 ]; then
    log "ERROR: No GPU devices found (/dev/nvidia[0-9]*)"
    ERRORS=$((ERRORS + 1))
    STATUS_NOTES="${STATUS_NOTES}No GPU devices found; "
else
    log "SUCCESS: Found $GPU_COUNT GPU device(s)"
fi

# Check 2: nvidia-smi command
log "Checking nvidia-smi..."
if ! command -v nvidia-smi &>/dev/null; then
    log "ERROR: nvidia-smi command not found"
    ERRORS=$((ERRORS + 1))
    STATUS_NOTES="${STATUS_NOTES}nvidia-smi not found; "
else
    # Try to run nvidia-smi
    if nvidia_output=$(nvidia-smi --query-gpu=name,memory.total,temperature.gpu,utilization.gpu --format=csv,noheader 2>&1); then
        log "SUCCESS: nvidia-smi working"
        log "GPU Status:"
        echo "$nvidia_output" | while IFS=',' read -r name memory temp util; do
            log "  - GPU: $name, Memory: $memory, Temp: $temp, Utilization: $util"
        done
        
        # Store GPU info in Consul
        if command -v consul &>/dev/null; then
            echo "$nvidia_output" | consul kv put "health/gpu/${CONTAINER_NAME}/devices" - >/dev/null 2>&1 || true
        fi
    else
        log "ERROR: nvidia-smi failed: $nvidia_output"
        ERRORS=$((ERRORS + 1))
        STATUS_NOTES="${STATUS_NOTES}nvidia-smi failed; "
    fi
fi

# Check 3: Docker GPU support (if Docker is installed)
if command -v docker &>/dev/null; then
    log "Checking Docker GPU support..."
    if docker run --rm --gpus all nvidia/cuda:12.0-base nvidia-smi &>/dev/null; then
        log "SUCCESS: Docker GPU support working"
    else
        log "WARNING: Docker GPU support not working"
        STATUS_NOTES="${STATUS_NOTES}Docker GPU support failed; "
    fi
fi

# Check 4: Memory and compute test
log "Running GPU memory test..."
if command -v nvidia-smi &>/dev/null; then
    # Get GPU memory info
    if gpu_memory=$(nvidia-smi --query-gpu=memory.free,memory.used,memory.total --format=csv,noheader,nounits 2>&1); then
        log "GPU Memory Status: $gpu_memory"
        
        # Check if any GPU has critically low memory (less than 10% free)
        echo "$gpu_memory" | while IFS=',' read -r free used total; do
            free_percent=$((free * 100 / total))
            if [ $free_percent -lt 10 ]; then
                log "WARNING: GPU memory critically low (${free_percent}% free)"
                STATUS_NOTES="${STATUS_NOTES}Low GPU memory; "
            fi
        done
    fi
fi

# Summary
if [ $ERRORS -eq 0 ]; then
    log "GPU health check PASSED - All GPUs are healthy"
    register_consul_health "passing" "All GPUs healthy"
    exit 0
else
    log "GPU health check FAILED - $ERRORS errors found"
    register_consul_health "critical" "$STATUS_NOTES"
    exit 1
fi