# Consul Server Configuration Template
# Purpose: This template configures a Consul server node that provides service discovery,
#          health checking, and KV storage for the ProposalsApp infrastructure.
# 
# Architecture Design:
# - Single Consul server per datacenter (no intra-DC clustering)
# - Three datacenters total: dc1 (sib-01), dc2 (sib-02), dc3 (sib-03)
# - WAN federation connects all three servers
# - Each server is the authoritative source for its local datacenter
#
# Why this design?
# With only 3 containers per physical server, if the server fails, all containers
# fail together. Intra-DC clustering would provide no real redundancy, so we use
# a simpler single-server-per-DC model with WAN federation for cross-DC coordination.

# Datacenter Configuration
# Each physical Proxmox server is its own datacenter
datacenter = "{{ datacenter }}"

# Primary datacenter handles ACL bootstrap and CA root certificate generation
# All secondary DCs will replicate ACL tokens and CA config from dc1
primary_datacenter = "dc1"

# Data persistence - survives container restarts
data_dir = "/var/lib/consul"

# Logging configuration
# INFO level provides good operational visibility without excessive noise
log_level = "INFO"

# Node identification - must be unique across entire Consul cluster
node_name = "{{ inventory_hostname }}"

# Server mode configuration
# This node participates in Raft consensus and stores service catalog
server = true

# Bootstrap configuration
# bootstrap_expect = 1 because we have exactly one server per datacenter
# This allows the server to self-elect as leader immediately
bootstrap_expect = 1

# Gossip encryption
# Prevents unauthorized nodes from joining the cluster
# This key must be identical across all Consul nodes (generated by consul keygen)
encrypt = "{{ consul_encrypt_key }}"

# Web UI Configuration
# Provides visual interface for debugging service discovery and KV store
ui_config {
  enabled = true
}

# Network binding configuration
# client_addr: API/UI/DNS interface - 0.0.0.0 allows access from any interface
# bind_addr: Cluster communication - uses Tailscale IP for encryption
# advertise_addr: What we tell other nodes to connect to (LAN)
# advertise_addr_wan: What we tell other DCs to connect to (WAN)
client_addr = "0.0.0.0"
bind_addr = "{{ tailscale_ip | default(ansible_tailscale0.ipv4.address | default(ansible_default_ipv4.address)) }}"
advertise_addr = "{{ tailscale_ip | default(ansible_tailscale0.ipv4.address | default(ansible_default_ipv4.address)) }}"
advertise_addr_wan = "{{ tailscale_ip | default(ansible_tailscale0.ipv4.address | default(ansible_default_ipv4.address)) }}"

# Single server per datacenter - no LAN join needed
# Each datacenter has exactly one Consul server

# WAN Federation Configuration
# Connects this server to Consul servers in other datacenters
# This enables cross-DC service discovery and KV replication
# The retry_join_wan ensures automatic reconnection after network issues
retry_join_wan = [
  {% for host in groups['consul_servers'] %}
  {% if host != inventory_hostname %}
  "{{ hostvars[host].tailscale_ip | default(hostvars[host].ansible_tailscale0.ipv4.address | default(hostvars[host].ansible_default_ipv4.address)) }}",
  {% endif %}
  {% endfor %}
]

# Port Configuration
ports {
  grpc = 8502      # gRPC API for Consul Connect (service mesh)
  dns = 8600       # DNS interface for service discovery (dig @localhost -p 8600 service.consul)
  http = 8500      # HTTP API and web UI
  https = -1       # HTTPS disabled (using Tailscale for encryption)
  serf_lan = 8301  # LAN gossip (not used in single-server DC)
  serf_wan = 8302  # WAN gossip between datacenters (critical for federation)
  server = 8300    # RPC port for Raft consensus
}

# Consul Connect Service Mesh
# Enables automatic mTLS and service authorization
connect {
  enabled = true
}

# Access Control Lists
# Currently disabled for operational simplicity
# In production, enable ACLs and use tokens for authentication
acl {
  enabled = false
  default_policy = "allow"
  enable_token_persistence = false
}

# Performance Tuning
# raft_multiplier = 1 uses default timing (suitable for <50ms latency)
# Our WAN links: Romania ↔ Romania ~10-20ms, Romania ↔ Germany ~30-50ms
performance {
  raft_multiplier = 1
}

# Telemetry Configuration
# Exposes Prometheus-compatible metrics on /v1/agent/metrics
telemetry {
  prometheus_retention_time = "60s"  # Keep metrics for 1 minute
  disable_hostname = true           # Cleaner metric names without hostname
}

# DNS Configuration
# Optimizations for service discovery via DNS
dns_config {
  enable_truncate = true   # Return partial results if response too large
  only_passing = true      # Only return healthy services in DNS responses
}

# Connection Limits
# Prevent single client from overwhelming the server
limits {
  http_max_conns_per_client = 200
  rpc_max_conns_per_client = 100
}

# Enable Script Checks
# Required for services that use script-based health checks
# enable_local_script_checks allows scripts defined in local service definitions
enable_local_script_checks = true