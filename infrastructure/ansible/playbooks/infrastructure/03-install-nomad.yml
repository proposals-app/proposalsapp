---
- name: Install and Configure Nomad Servers
  hosts: nomad_servers
  become: true
  tasks:
    - name: Set connection to use tailscale IP if available
      set_fact:
        ansible_host: "{{ tailscale_ip }}"
      when: tailscale_ip is defined and tailscale_ip != ''
    - name: Install Nomad
      apt:
        name: nomad
        state: present

    - name: Create Nomad server configuration
      template:
        src: ../../templates/nomad-server.hcl.j2
        dest: /etc/nomad.d/nomad.hcl
        owner: nomad
        group: nomad
        mode: "0640"
      notify: restart nomad

    - name: Create systemd override directory for Nomad
      file:
        path: /etc/systemd/system/nomad.service.d
        state: directory
        mode: "0755"

    - name: Create Nomad systemd service override
      copy:
        content: |
          [Unit]
          # Ensure Consul is available
          After=consul.service
          Wants=consul.service

          [Service]
          Environment="CONSUL_HTTP_ADDR=http://127.0.0.1:{{ consul_client_port }}"
          # Add startup delay to ensure Consul is ready
          ExecStartPre=/bin/sleep 5
        dest: /etc/systemd/system/nomad.service.d/override.conf
      notify:
        - reload systemd
        - restart nomad

    - name: Start and enable Nomad
      systemd:
        name: nomad
        state: started
        enabled: yes
        daemon_reload: yes

    - name: Wait for Nomad to be ready
      wait_for:
        port: "{{ nomad_http_port }}"
        delay: 5

    - name: Wait for leader election
      shell: |
        nomad operator raft list-peers | grep -q "leader"
      register: leader_check
      until: leader_check.rc == 0
      retries: 10
      delay: 5
      run_once: true
      changed_when: false

  handlers:
    - name: reload systemd
      systemd:
        daemon_reload: yes

    - name: restart nomad
      systemd:
        name: nomad
        state: restarted

- name: Install and Configure Nomad Clients
  hosts: nomad_clients
  become: true
  tasks:
    - name: Set connection to use tailscale IP if available
      set_fact:
        ansible_host: "{{ tailscale_ip }}"
      when: tailscale_ip is defined and tailscale_ip != ''
    - name: Install Nomad
      apt:
        name: nomad
        state: present

    - name: Install Docker
      apt:
        name:
          - docker.io
          - docker-compose
        state: present

    - name: Add nomad user to docker group
      user:
        name: nomad
        groups: docker
        append: yes

    - name: Create Nomad client configuration
      template:
        src: ../../templates/nomad-client.hcl.j2
        dest: /etc/nomad.d/nomad.hcl
        owner: nomad
        group: nomad
        mode: "0640"
      notify: restart nomad

    - name: Create systemd override directory for Nomad
      file:
        path: /etc/systemd/system/nomad.service.d
        state: directory
        mode: "0755"

    - name: Create Nomad systemd service override
      copy:
        content: |
          [Unit]
          # Ensure Consul is available
          After=consul.service
          Wants=consul.service

          [Service]
          Environment="CONSUL_HTTP_ADDR=http://127.0.0.1:{{ consul_client_port }}"
          # Add startup delay to ensure Consul is ready
          ExecStartPre=/bin/sleep 5
        dest: /etc/systemd/system/nomad.service.d/override.conf
      notify:
        - reload systemd
        - restart nomad

    - name: Create additional Nomad client dependencies override
      copy:
        content: |
          [Unit]
          # Additional dependencies for Nomad clients - made non-fatal to avoid circular dependencies
          After=pgpool.service haproxy.service consul.service

          [Service]
          # Wait for Consul to be ready first (critical dependency)
          ExecStartPre=/bin/bash -c 'for i in {1..30}; do consul members &>/dev/null && echo "Consul is ready" && exit 0 || echo "Waiting for Consul to be ready..."; sleep 2; done; exit 1'

          # Try to wait for pgpool, but don't block startup if it's not available
          # This prevents circular dependency - Nomad can start and then schedule pgpool
          ExecStartPre=-/bin/bash -c 'echo "[Nomad] Checking for pgpool service in Consul..."; for i in {1..10}; do consul catalog services 2>/dev/null | grep -q pgpool && echo "[Nomad] pgpool found in Consul" && exit 0; echo "[Nomad] Waiting for pgpool in Consul (non-fatal)..."; sleep 2; done; echo "[Nomad] pgpool not found in Consul after 20s, continuing anyway (will be scheduled by Nomad)"; exit 0'

          # Try to wait for haproxy-redis, but don't block startup if it's not available
          ExecStartPre=-/bin/bash -c 'echo "[Nomad] Checking for haproxy-redis service in Consul..."; for i in {1..10}; do consul catalog services 2>/dev/null | grep -q haproxy-redis && echo "[Nomad] haproxy-redis found in Consul" && exit 0; echo "[Nomad] Waiting for haproxy-redis in Consul (non-fatal)..."; sleep 2; done; echo "[Nomad] haproxy-redis not found in Consul after 20s, continuing anyway (will be scheduled by Nomad)"; exit 0'
        dest: /etc/systemd/system/nomad.service.d/client-dependencies.conf
      when: inventory_hostname in groups['nomad_clients']
      notify:
        - reload systemd
        - restart nomad

    - name: Create host volume directories
      file:
        path: "{{ item }}"
        state: directory
        owner: nomad
        group: nomad
        mode: "0755"
      loop:
        - /opt/nomad-volumes
        - /opt/nomad-volumes/loki
        - /opt/nomad-volumes/grafana
        - /opt/nomad-volumes/prometheus
        - /var/lib/nomad-volumes

    - name: Install NVIDIA Container Toolkit (GPU-enabled containers only)
      when: inventory_hostname in gpu_passthrough_containers | default([])
      block:
        - name: Create keyring directory
          file:
            path: /usr/share/keyrings
            state: directory
            mode: "0755"

        - name: Add NVIDIA GPG key to keyring
          get_url:
            url: https://nvidia.github.io/libnvidia-container/gpgkey
            dest: /tmp/nvidia-gpgkey
          register: nvidia_gpgkey

        - name: Dearmor NVIDIA GPG key
          shell: gpg --dearmor < /tmp/nvidia-gpgkey > /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
          when: nvidia_gpgkey is changed

        - name: Add NVIDIA container toolkit repository with signed-by
          shell: |
            curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
            sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
            tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
          args:
            creates: /etc/apt/sources.list.d/nvidia-container-toolkit.list

        - name: Update apt cache
          apt:
            update_cache: yes

        - name: Install NVIDIA container toolkit
          apt:
            name:
              - nvidia-container-toolkit
            state: latest

        - name: Add NVIDIA CUDA repository for libraries
          shell: |
            wget -qO- https://developer.download.nvidia.com/compute/cuda/repos/debian12/x86_64/3bf863cc.pub | apt-key add -
            echo "deb https://developer.download.nvidia.com/compute/cuda/repos/debian12/x86_64/ /" > /etc/apt/sources.list.d/cuda-debian12.list
          args:
            creates: /etc/apt/sources.list.d/cuda-debian12.list

        - name: Update apt cache for CUDA repo
          apt:
            update_cache: yes

        - name: Install NVIDIA libraries
          apt:
            name:
              - cuda-drivers
              - libnvidia-ml-dev
            state: latest
          ignore_errors: yes # May partially fail but installs needed libraries

        - name: Create symlink for libnvidia-ml.so
          shell: |
            # Find the latest libnvidia-ml.so.* file
            NVIDIA_ML_LIB=$(ls -1 /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.* 2>/dev/null | sort -V | tail -1)
            if [ -n "$NVIDIA_ML_LIB" ]; then
              cd /usr/lib/x86_64-linux-gnu
              ln -sf "$(basename $NVIDIA_ML_LIB)" libnvidia-ml.so
              ln -sf "$(basename $NVIDIA_ML_LIB)" libnvidia-ml.so.1
              ldconfig
            fi
          args:
            creates: /usr/lib/x86_64-linux-gnu/libnvidia-ml.so

    - name: Create Docker daemon configuration
      copy:
        content: |
          {
            "log-driver": "json-file",
            "log-opts": {
              "max-size": "10m",
              "max-file": "3"
            },
            "storage-driver": "overlay2",
            "metrics-addr": "127.0.0.1:9323",
            "experimental": true{% if inventory_hostname in gpu_passthrough_containers | default([]) %},
            "runtimes": {
              "nvidia": {
                "path": "nvidia-container-runtime",
                "runtimeArgs": []
              }
            },
            "default-runtime": "nvidia"{% endif %}
          }
        dest: /etc/docker/daemon.json
      notify: restart docker

    - name: Install Nomad GPU device plugin (GPU-enabled containers only)
      when: inventory_hostname in gpu_passthrough_containers | default([])
      block:
        - name: Create Nomad plugins directory
          file:
            path: /opt/nomad/plugins
            state: directory
            owner: nomad
            group: nomad
            mode: "0755"

        - name: Download Nomad nvidia-gpu device plugin
          unarchive:
            src: https://releases.hashicorp.com/nomad-device-nvidia/1.1.0/nomad-device-nvidia_1.1.0_linux_amd64.zip
            dest: /opt/nomad/plugins/
            remote_src: yes
            owner: nomad
            group: nomad
            mode: "0755"
            creates: /opt/nomad/plugins/nomad-device-nvidia

        - name: Rename nvidia-gpu plugin binary
          command: mv /opt/nomad/plugins/nomad-device-nvidia /opt/nomad/plugins/nvidia-gpu
          args:
            creates: /opt/nomad/plugins/nvidia-gpu

    - name: Create Docker authentication for GitHub Container Registry
      copy:
        content: |
          {
            "auths": {
              "ghcr.io": {
                "auth": "{{ (vault_github_org + ':' + vault_github_pat) | b64encode }}"
              }
            }
          }
        dest: /etc/docker/config.json
        mode: "0600"
        owner: root
        group: root
      when: vault_github_pat is defined and vault_github_pat != ""
      notify: restart nomad

    - name: Start and enable services
      systemd:
        name: "{{ item }}"
        state: started
        enabled: yes
        daemon_reload: yes
      loop:
        - docker
        - nomad

  handlers:
    - name: reload systemd
      systemd:
        daemon_reload: yes

    - name: restart nomad
      systemd:
        name: nomad
        state: restarted

    - name: restart docker
      systemd:
        name: docker
        state: restarted

# Verification checks at the end
- name: Verify Nomad Cluster
  hosts: nomad_servers
  gather_facts: false
  tasks:
    - name: Wait for Nomad to stabilize
      pause:
        seconds: 10

    - name: Check Nomad server health
      shell: |
        # Check if Nomad is running
        if ! systemctl is-active nomad >/dev/null 2>&1; then
          echo "ERROR: Nomad service is not running"
          exit 1
        fi

        # Check server members
        members=$(nomad server members 2>&1)
        if [ $? -ne 0 ]; then
          echo "ERROR: Cannot query Nomad members: $members"
          exit 1
        fi

        # Count alive servers
        alive_servers=$(echo "$members" | grep -c "alive.*true")
        if [ $alive_servers -lt 1 ]; then
          echo "ERROR: No alive Nomad servers found"
          exit 1
        fi

        echo "OK: Found $alive_servers alive Nomad servers"
        echo "$members"
      register: nomad_health
      changed_when: false
      failed_when: nomad_health.rc != 0

    - name: Check Nomad leader election
      shell: |
        if ! nomad operator raft list-peers 2>&1 | grep -q "leader"; then
          echo "ERROR: No Nomad leader elected"
          exit 1
        fi
        nomad operator raft list-peers
      register: nomad_leader
      changed_when: false
      run_once: true

    - name: Check Nomad regions
      shell: |
        regions=$(nomad server members 2>&1)
        if [ $? -ne 0 ]; then
          echo "ERROR: Cannot query server members: $regions"
          exit 1
        fi
        echo "OK: Nomad servers configured"
        echo "$regions"
      register: nomad_regions
      changed_when: false
      run_once: true

    - name: Display Nomad server verification summary
      debug:
        msg: |
          ========================================
          Nomad Cluster Verification Summary
          ========================================
          Service Status: {{ 'RUNNING' if 'OK' in nomad_health.stdout else 'FAILED' }}
          Leader Election: {{ 'YES' if nomad_leader.rc == 0 else 'NO' }}

          Server Members:
          {{ nomad_health.stdout | indent(2) }}

          Regions:
          {{ nomad_regions.stdout | indent(2) }}

          Access Nomad UI at: http://{{ tailscale_ip | default(ansible_default_ipv4.address) }}:4646
          ========================================
      run_once: true

- name: Verify Nomad Clients
  hosts: nomad_clients
  gather_facts: false
  tasks:
    - name: Check Nomad client health
      shell: |
        # Check if Nomad is running
        if ! systemctl is-active nomad >/dev/null 2>&1; then
          echo "ERROR: Nomad service is not running"
          exit 1
        fi

        # Check node status
        node_status=$(nomad node status -self -short 2>&1)
        if [ $? -ne 0 ]; then
          echo "ERROR: Cannot query node status: $node_status"
          exit 1
        fi

        # Check if node is ready
        if ! echo "$node_status" | grep -q "ready"; then
          echo "ERROR: Node is not ready"
          echo "$node_status"
          exit 1
        fi

        echo "OK: Nomad client ready"
        echo "$node_status"
      register: client_health
      changed_when: false
      failed_when: client_health.rc != 0

    - name: Check Docker availability
      shell: |
        if ! systemctl is-active docker >/dev/null 2>&1; then
          echo "ERROR: Docker service is not running"
          exit 1
        fi

        if ! docker info >/dev/null 2>&1; then
          echo "ERROR: Cannot connect to Docker daemon"
          exit 1
        fi

        echo "OK: Docker is available"
      register: docker_health
      changed_when: false
      failed_when: docker_health.rc != 0

    - name: Check Consul integration
      shell: |
        # Check if Consul agent is running and accessible
        if ! consul members | grep -q "{{ inventory_hostname }}.*alive"; then
          echo "ERROR: Consul integration not working"
          exit 1
        fi

        echo "OK: Consul integration working"
      register: consul_integration
      changed_when: false
      failed_when: consul_integration.rc != 0

    - name: Display client verification summary
      debug:
        msg: |
          ========================================
          Nomad Client Verification - {{ inventory_hostname }}
          ========================================
          Client Status: {{ 'READY' if client_health.rc == 0 else 'FAILED' }}
          Docker Status: {{ 'RUNNING' if docker_health.rc == 0 else 'FAILED' }}
          Consul Integration: {{ 'WORKING' if consul_integration.rc == 0 else 'FAILED' }}

          Node Info:
          {{ client_health.stdout | indent(2) }}
          ========================================
