---
# This playbook combines container provisioning and base setup into a single operation
# It first creates LXC containers on Proxmox nodes, then prepares them with base packages

- name: Create LXC Containers on Proxmox
  hosts: proxmox_nodes
  gather_facts: true
  pre_tasks:
    - name: Clean all SSH known hosts for containers before provisioning
      shell: |
        # Clean up any old backup files
        rm -f ~/.ssh/known_hosts.old 2>/dev/null || true

        # Remove all container-related entries
        if [ -f ~/.ssh/known_hosts ]; then
          # Remove Tailscale IP range (100.x.x.x)
          grep -v "^100\." ~/.ssh/known_hosts > ~/.ssh/known_hosts.tmp 2>/dev/null || true
          mv ~/.ssh/known_hosts.tmp ~/.ssh/known_hosts 2>/dev/null || true

          # Remove container network range (10.10.x.x)
          grep -v "^10\.10\." ~/.ssh/known_hosts > ~/.ssh/known_hosts.tmp 2>/dev/null || true
          mv ~/.ssh/known_hosts.tmp ~/.ssh/known_hosts 2>/dev/null || true

          # Remove known container hostnames
          for host in apps-sib-01 apps-sib-03 apps-fsn-01 consul-nomad-sib-01 consul-nomad-sib-03 consul-nomad-fsn-01 db-sib-01 db-sib-03 db-fsn-01 redis-sib-01 redis-sib-03 redis-fsn-01 github-runner-sib-01 github-runner-sib-03 github-runner-fsn-01 pgbackweb-sib-01; do
            ssh-keygen -f ~/.ssh/known_hosts -R "$host" 2>/dev/null || true
          done
        fi
      delegate_to: localhost
      run_once: true
      changed_when: false
  vars:
    proxmox_credentials:
      sib-01: "{{ vault_proxmox_password_sib01 }}"
      sib-03: "{{ vault_proxmox_password_sib03 }}"
      fsn-01: "{{ vault_proxmox_password_fsn01 }}"
    # Container IDs use schema: 5[dc_id][01-99]
    # dc_id: 1=sib-01(dc1), 2=sib-03(dc2), 3=fsn-01(dc3)
    # Sequential numbering: 01, 02, 03, 04, 05 for each container type
    container_id_base:
      sib-01: 5100 # Will use 5101, 5102, 5103, 5104, 5105
      sib-03: 5200 # Will use 5201, 5202, 5203, 5204, 5205
      fsn-01: 5300 # Will use 5301, 5302, 5303, 5304, 5305
    # Determine bridge based on location
    network_bridge:
      sib-01: sibnet
      sib-03: sibnet
      fsn-01: sibnet
    lxc_defaults:
      consul_nomad:
        cores: 2
        memory: 4096
        swap: 0
        disk: 20
      database:
        cores: 4
        memory: 8192
        swap: 0
        disk: 100
      apps:
        cores: 8
        memory: 32768
        swap: 0
        disk: 50
      redis:
        cores: 4
        memory: 8192
        swap: 0
        disk: 30
      github_runner:
        cores: 8
        memory: 16384
        swap: 0
        disk: 100
      github_runner_sib: # Double resources for Sibiu runners
        cores: 16
        memory: 32768
        swap: 0
        disk: 100
      pgbackweb:
        cores: 2
        memory: 4096
        swap: 0
        disk: 200
  tasks:
    - name: Install Python3 and pip
      apt:
        name:
          - python3
          - python3-pip
        state: present
        update_cache: yes

    - name: Get list of existing containers
      shell: "pct list | awk 'NR>1 {print $1}' | sort -n"
      register: existing_containers
      changed_when: false

    - name: Set container IDs based on hostname
      set_fact:
        my_containers: "{{ base_containers + (pgbackweb_container if inventory_hostname == 'sib-01' else []) }}"
      vars:
        base_containers:
          - name: "consul-nomad-{{ inventory_hostname }}"
            id: "{{ container_id_base[inventory_hostname] + 1 }}"
            type: consul_nomad
          - name: "db-{{ inventory_hostname }}"
            id: "{{ container_id_base[inventory_hostname] + 2 }}"
            type: database
          - name: "apps-{{ inventory_hostname }}"
            id: "{{ container_id_base[inventory_hostname] + 3 }}"
            type: apps
          - name: "github-runner-{{ inventory_hostname }}"
            id: "{{ container_id_base[inventory_hostname] + 4 }}"
            type: "{{ 'github_runner_sib' if inventory_hostname in ['sib-01', 'sib-03'] else 'github_runner' }}"
          - name: "redis-{{ inventory_hostname }}"
            id: "{{ container_id_base[inventory_hostname] + 5 }}"
            type: redis
        pgbackweb_container:
          - name: "pgbackweb-{{ inventory_hostname }}"
            id: "{{ container_id_base[inventory_hostname] + 7 }}"
            type: pgbackweb

    - name: Check if our containers exist
      shell: "pct list | grep -E '^{{ item.id }}\\s' || echo 'not_found'"
      register: container_exists
      loop: "{{ my_containers }}"
      changed_when: false

    - name: Create containers
      shell: |
        pct create {{ item.0.id }} \
          local:vztmpl/debian-12-standard_12.7-1_amd64.tar.zst \
          --hostname {{ item.0.name }} \
          --cores {{ lxc_defaults[item.0.type].cores }} \
          --memory {{ lxc_defaults[item.0.type].memory }} \
          --swap {{ lxc_defaults[item.0.type].swap }} \
          --rootfs zfs-ssd-m2:{{ lxc_defaults[item.0.type].disk }} \
          --net0 name=eth0,bridge={{ network_bridge[inventory_hostname] }},firewall=1,ip=dhcp,type=veth \
          --nameserver 8.8.8.8 \
          --features nesting=1,fuse=1 \
          --unprivileged 1 \
          --onboot 1 \
          --ssh-public-keys /root/.ssh/authorized_keys \
          --start 0
      when: "'not_found' in item.1.stdout"
      loop: "{{ my_containers | zip(container_exists.results) | list }}"
      register: container_created

    - name: Configure Tailscale /dev/tun access in containers
      shell: |
        echo "Configuring /dev/tun for container {{ item.0.id }}..."

        # For unprivileged containers, we need different device permissions
        if ! grep -q 'lxc.cgroup2.devices.allow: c 10:200 rwm' /etc/pve/lxc/{{ item.0.id }}.conf; then
          echo 'lxc.cgroup2.devices.allow: c 10:200 rwm' >> /etc/pve/lxc/{{ item.0.id }}.conf
          echo 'Added cgroup2 device access'
        else
          echo 'cgroup2 device access already configured'
        fi

        if ! grep -q 'lxc.mount.entry: /dev/net/tun dev/net/tun none bind,create=file' /etc/pve/lxc/{{ item.0.id }}.conf; then
          echo 'lxc.mount.entry: /dev/net/tun dev/net/tun none bind,create=file' >> /etc/pve/lxc/{{ item.0.id }}.conf
          echo 'Added mount entry for /dev/net/tun'
        else
          echo 'Mount entry already configured'
        fi

        # Add ID mapping for unprivileged containers
        if ! grep -q 'lxc.idmap' /etc/pve/lxc/{{ item.0.id }}.conf; then
          echo 'lxc.idmap: u 0 100000 65536' >> /etc/pve/lxc/{{ item.0.id }}.conf
          echo 'lxc.idmap: g 0 100000 65536' >> /etc/pve/lxc/{{ item.0.id }}.conf
        fi

        echo "Current config:"
        grep -E '(cgroup2.devices.allow|mount.entry.*tun)' /etc/pve/lxc/{{ item.0.id }}.conf || echo "No tun config found!"
      when: "'not_found' in item.1.stdout"
      loop: "{{ my_containers | zip(container_exists.results) | list }}"
      register: tun_config

    - name: Check container status
      shell: "pct status {{ item.id }} | grep -q 'status: running' && echo 'running' || echo 'stopped'"
      loop: "{{ my_containers }}"
      register: container_status
      changed_when: false

    - name: Start containers
      shell: "pct start {{ item.0.id }}"
      loop: "{{ my_containers | zip(container_status.results) | list }}"
      when: "'stopped' in item.1.stdout"
      register: containers_started

    - name: Wait for containers to be ready
      pause:
        seconds: 15
      when: container_created.changed

    - name: Enable SSH in containers
      shell: |
        pct exec {{ item.id }} -- bash -c "
          # Wait for network
          sleep 5

          # Update package list
          apt-get update

          # Install SSH server and other essentials
          apt-get install -y openssh-server curl

          # Configure SSH
          mkdir -p /root/.ssh
          chmod 700 /root/.ssh

          # Add your SSH key
          echo '{{ lookup('file', '~/.ssh/id_rsa.pub', errors='ignore') }}' > /root/.ssh/authorized_keys
          chmod 600 /root/.ssh/authorized_keys

          # Configure SSH daemon
          sed -i 's/#PermitRootLogin.*/PermitRootLogin yes/' /etc/ssh/sshd_config
          sed -i 's/#PubkeyAuthentication.*/PubkeyAuthentication yes/' /etc/ssh/sshd_config

          # Enable and start SSH
          systemctl enable ssh
          systemctl restart ssh
        "
      loop: "{{ my_containers }}"
      ignore_errors: true
      when: container_created.changed

    - name: Install and start Tailscale in containers
      shell: |
        pct exec {{ item.id }} -- bash -c "
          echo '=== Setting up Tailscale in {{ item.name }} ==='

          # Check if Tailscale is already running and configured
          if systemctl is-active --quiet tailscaled && tailscale status &>/dev/null; then
            echo 'Tailscale is already installed and running'
            echo 'Current status:'
            tailscale ip -4 2>/dev/null || echo 'No IP assigned yet'
            exit 0
          fi

          # Verify /dev/net/tun exists
          echo 'Checking /dev/net/tun...'
          if [ -e /dev/net/tun ]; then
            echo '/dev/net/tun exists and is properly configured'
            ls -la /dev/net/tun
          else
            echo 'ERROR: /dev/net/tun does NOT exist!'
            echo 'This indicates the container was not properly configured.'
            echo 'The container needs to be stopped and restarted with proper TUN configuration.'
            exit 1
          fi

          # Check if Tailscale is already installed
          if ! command -v tailscale &> /dev/null; then
            echo 'Installing Tailscale...'
            curl -fsSL https://tailscale.com/install.sh | sh
          else
            echo 'Tailscale already installed'
          fi

          # Ensure Tailscale uses standard mode (not userspace)
          echo 'Configuring Tailscale for standard mode...'
          if [ -f /etc/default/tailscaled ]; then
            # Remove any userspace mode flags
            sed -i 's/FLAGS=".*--tun=userspace-networking.*"/FLAGS=""/' /etc/default/tailscaled
            # Ensure FLAGS is empty or doesn't contain userspace flag
            if ! grep -q '^FLAGS=""' /etc/default/tailscaled; then
              echo 'FLAGS=""' >> /etc/default/tailscaled
            fi
          else
            # Create the file with empty FLAGS
            echo 'FLAGS=""' > /etc/default/tailscaled
          fi

          # Enable IP forwarding
          if ! grep -q 'net.ipv4.ip_forward=1' /etc/sysctl.conf; then
            echo 'net.ipv4.ip_forward=1' >> /etc/sysctl.conf
          fi
          if ! grep -q 'net.ipv6.conf.all.forwarding=1' /etc/sysctl.conf; then
            echo 'net.ipv6.conf.all.forwarding=1' >> /etc/sysctl.conf
          fi
          sysctl -p

          # Start and enable tailscaled
          echo 'Starting tailscaled...'
          systemctl enable tailscaled
          systemctl start tailscaled

          # Wait for daemon to be ready
          for i in {1..10}; do
            if systemctl is-active --quiet tailscaled; then
              echo 'Tailscale daemon is running'
              break
            fi
            echo 'Waiting for tailscaled to start...'
            sleep 2
          done

          # Verify it's running
          systemctl status tailscaled --no-pager || true

          # Check logs for any errors
          echo -e '\nChecking tailscaled logs for errors...'
          journalctl -u tailscaled -n 20 --no-pager | grep -i error || echo 'No errors in logs'
        "
      loop: "{{ my_containers }}"
      ignore_errors: true
      register: tailscale_install

    - name: Check if Tailscale is already configured
      shell: |
        pct exec {{ item.id }} -- bash -c "
          # Check if already connected and configured
          if tailscale status &>/dev/null && tailscale ip -4 &>/dev/null; then
            echo 'already_configured'
          else
            echo 'needs_configuration'
          fi
        "
      loop: "{{ my_containers }}"
      register: tailscale_status_check
      changed_when: false
      ignore_errors: true

    - name: Configure Tailscale with auth key
      shell: |
        pct exec {{ item.0.id }} -- bash -c "
          echo '=== Configuring Tailscale for {{ item.0.name }} ==='

          # Check if tailscaled is running
          if ! systemctl is-active --quiet tailscaled; then
            echo 'ERROR: tailscaled is not running!'
            systemctl status tailscaled --no-pager
            exit 1
          fi

          # Configure Tailscale with hostname
          echo 'Running: tailscale up --authkey=REDACTED --hostname={{ item.0.name }} --accept-routes'
          tailscale up --authkey={{ vault_tailscale_auth_key }} --hostname={{ item.0.name }} --accept-routes 2>&1

          # Check the result
          echo 'Exit code:' $?
        "
      loop: "{{ my_containers | zip(tailscale_status_check.results) | list }}"
      when:
        - vault_tailscale_auth_key is defined
        - "'needs_configuration' in item.1.stdout"
      register: tailscale_setup

    - name: Wait for Tailscale to connect
      pause:
        seconds: 20
      when:
        - vault_tailscale_auth_key is defined
        - tailscale_setup is defined
        - tailscale_setup.changed

    - name: Verify tailscale0 interface exists
      shell: |
        pct exec {{ item.id }} -- bash -c "
          echo '=== Verifying tailscale0 interface for {{ item.name }} ==='
          
          # Wait for interface to come up
          for i in {1..10}; do
            if ip link show tailscale0 &>/dev/null; then
              echo 'tailscale0 interface exists!'
              ip addr show tailscale0
              exit 0
            fi
            echo 'Waiting for tailscale0 interface... (attempt $i/10)'
            sleep 2
          done
          
          echo 'ERROR: tailscale0 interface not created!'
          echo 'This may indicate Tailscale is running in userspace mode.'
          echo 'Checking tailscaled process flags...'
          ps aux | grep tailscaled | grep -v grep || echo 'tailscaled not running'
          exit 1
        "
      loop: "{{ my_containers }}"
      when: vault_tailscale_auth_key is defined
      register: tailscale_interface_check

    - name: Check Tailscale status in all containers
      shell: |
        pct exec {{ item.id }} -- bash -c "
          echo '=== Tailscale Status for {{ item.name }} ==='
          echo 'Checking if tailscaled is running...'
          systemctl is-active tailscaled || echo 'tailscaled is NOT active'

          echo -e '\nChecking Tailscale status...'
          tailscale status 2>&1 || echo 'Failed to get status'

          echo -e '\nChecking Tailscale IP...'
          tailscale ip -4 2>&1 || echo 'No IP assigned'

          echo -e '\nChecking network interfaces...'
          ip addr show tailscale0 2>&1 || echo 'No tailscale0 interface'

          echo -e '\nChecking if logged in...'
          tailscale status --json 2>/dev/null | jq -r '.BackendState' || echo 'Cannot get backend state'
        "
      loop: "{{ my_containers }}"
      register: tailscale_status
      when: vault_tailscale_auth_key is defined

    - name: Configure DNS for Tailscale hostnames
      shell: |
        pct exec {{ item.id }} -- bash -c "
          # Add tailscale DNS to resolv.conf if not already present
          if ! grep -q '{{ dns_servers.tailscale_magic_dns }}' /etc/resolv.conf; then
            echo 'nameserver {{ dns_servers.tailscale_magic_dns }}' > /etc/resolv.conf.new
            cat /etc/resolv.conf >> /etc/resolv.conf.new
            mv /etc/resolv.conf.new /etc/resolv.conf
          fi
        "
      loop: "{{ my_containers }}"
      when: vault_tailscale_auth_key is defined

    - name: Capture Tailscale IPs for all containers
      shell: |
        pct exec {{ item.id }} -- bash -c "tailscale ip -4 2>/dev/null || echo ''"
      loop: "{{ my_containers }}"
      register: container_tailscale_ips
      when: vault_tailscale_auth_key is defined

    - name: Clean up old known hosts backup
      file:
        path: "~/.ssh/known_hosts.old"
        state: absent
      delegate_to: localhost
      run_once: true

    - name: Remove old SSH known hosts entries for containers
      known_hosts:
        name: "{{ item.name }}"
        state: absent
      loop: "{{ my_containers }}"
      delegate_to: localhost
      run_once: false
      ignore_errors: true

    - name: Create facts directory
      file:
        path: /etc/ansible/facts.d
        state: directory
        mode: "0755"

    - name: Store container information as facts
      copy:
        content: |
          {
            "containers": {
          {% for idx in range(my_containers | length) %}
          {% set container = my_containers[idx] %}
          {% set tailscale_ip = container_tailscale_ips.results[idx].stdout | default('') if container_tailscale_ips is defined else '' %}
              "{{ container.name }}": {
                "id": "{{ container.id }}",
                "hostname": "{{ container.name }}",
                "proxmox_node": "{{ inventory_hostname }}",
                "tailscale_ip": "{{ tailscale_ip | trim }}"
              }{% if not loop.last %},{% endif %}
          {% endfor %}
            }
          }
        dest: /etc/ansible/facts.d/lxc_containers.fact
        mode: "0644"

    - name: Store fresh Tailscale IPs for containers
      set_fact:
        fresh_container_ips: >-
          {%- set ips = {} -%}
          {%- for idx in range(my_containers | length) -%}
            {%- set container = my_containers[idx] -%}
            {%- set tailscale_ip = container_tailscale_ips.results[idx].stdout | default('') | trim if container_tailscale_ips is defined else '' -%}
            {%- set _ = ips.update({container.name: tailscale_ip}) -%}
          {%- endfor -%}
          {{ ips }}
      when: container_tailscale_ips is defined

    - name: Final container information
      debug:
        msg: |
          ========================================
          Container Setup Complete on {{ inventory_hostname }}
          ========================================
          {% for idx in range(my_containers | length) %}
          {% set container = my_containers[idx] %}
          {% set tailscale_ip = container_tailscale_ips.results[idx].stdout | default('') | trim if container_tailscale_ips is defined else '' %}
          {{ container.name }}:
            - Container ID: {{ container.id }}
            - Tailscale hostname: {{ container.name }}
            - Tailscale IP: {{ tailscale_ip if tailscale_ip else 'Not assigned' }}
            - Bridge: {{ network_bridge[inventory_hostname] }}
          {% endfor %}

          {% if vault_tailscale_auth_key is defined %}
          Tailscale has been configured. Containers are accessible via:
          {% for idx in range(my_containers | length) %}
          {% set container = my_containers[idx] %}
          {% set tailscale_ip = container_tailscale_ips.results[idx].stdout | default('') | trim if container_tailscale_ips is defined else '' %}
            ssh root@{{ container.name }} ({{ tailscale_ip if tailscale_ip else 'IP pending' }})
          {% endfor %}
          {% else %}
          WARNING: Tailscale auth key not found in vault!
          Add vault_tailscale_auth_key to your vault file:
          ansible-vault edit group_vars/all/vault.yml --vault-password-file vault-password.txt
          {% endif %}
          ========================================

- name: Update inventory with Tailscale IPs
  hosts: proxmox_nodes
  gather_facts: false
  tasks:
    - name: Read current inventory
      slurp:
        src: "{{ playbook_dir }}/../../inventory.yml"
      register: inventory_content
      delegate_to: localhost
      run_once: true

    - name: Parse inventory
      set_fact:
        inventory_data: "{{ inventory_content.content | b64decode | from_yaml }}"
      delegate_to: localhost
      run_once: true

    - name: Update inventory with Tailscale IPs
      set_fact:
        updated_inventory: |
          {% set inv = inventory_data %}
          {%- for host in groups['proxmox_nodes'] -%}
            {%- if hostvars[host]['container_tailscale_ips'] is defined -%}
              {%- for idx in range(hostvars[host]['my_containers'] | length) -%}
                {%- set container = hostvars[host]['my_containers'][idx] -%}
                {%- set ip = hostvars[host]['container_tailscale_ips']['results'][idx]['stdout'] | default('') | trim -%}
                {%- if ip and container.name in inv.all.children.lxc_containers.hosts -%}
                  {%- set _ = inv.all.children.lxc_containers.hosts[container.name].update({'tailscale_ip': ip}) -%}
                {%- endif -%}
              {%- endfor -%}
            {%- endif -%}
          {%- endfor -%}
          {{ inv | to_nice_yaml(indent=2) }}
      delegate_to: localhost
      run_once: true

    - name: Write updated inventory
      copy:
        content: "{{ updated_inventory }}"
        dest: "{{ playbook_dir }}/../../inventory.yml"
        backup: false
        mode: "0644"
      delegate_to: localhost
      run_once: true

# Now run base setup on all newly created containers
- name: Base Setup for All LXC Containers
  hosts: lxc_containers
  become: true
  gather_facts: false
  tasks:
    - name: Get fresh Tailscale IP from proxmox hosts
      set_fact:
        fresh_ip: "{{ hostvars[proxmox_node]['fresh_container_ips'][inventory_hostname] }}"
      when:
        - hostvars[proxmox_node]['fresh_container_ips'] is defined
        - inventory_hostname in hostvars[proxmox_node]['fresh_container_ips']

    - name: Use fresh Tailscale IP for connection
      set_fact:
        ansible_host: "{{ fresh_ip }}"
      when: fresh_ip is defined and fresh_ip != ''

    - name: Remove old SSH known hosts entries for all containers
      shell: |
        # Remove any existing backup file first
        rm -f ~/.ssh/known_hosts.old 2>/dev/null || true
        # Now remove the host entry
        ssh-keygen -f ~/.ssh/known_hosts -R "{{ item }}" 2>/dev/null || true
      loop:
        - "{{ inventory_hostname }}"
        - "{{ tailscale_ip | default('') }}"
        - "{{ fresh_ip | default('') }}"
        - "{{ ansible_host | default('') }}"
      when: item != ''
      delegate_to: localhost
      become: false
      changed_when: false
      ignore_errors: true # Continue even if ssh-keygen fails

    - name: Wait for container network to be ready
      wait_for:
        host: "{{ ansible_host | default(inventory_hostname) }}"
        port: 22
        delay: 5
        timeout: 60
      delegate_to: localhost
      become: false

    - name: Wait for SSH connection
      wait_for_connection:
        delay: 5
        timeout: 180

    - name: Gather facts after connection
      setup:

    - name: Update apt cache
      apt:
        update_cache: yes
        cache_valid_time: 3600

    - name: Install base packages
      apt:
        name:
          - curl
          - wget
          - gnupg
          - lsb-release
          - ca-certificates
          - apt-transport-https
          - software-properties-common
          - python3-pip
          - git
          - vim
          - htop
          - net-tools
          - dnsutils
          - jq
          - unzip
          - rsync
        state: present

    - name: Set timezone
      timezone:
        name: UTC

    - name: Configure sysctl for better performance
      sysctl:
        name: "{{ item.name }}"
        value: "{{ item.value }}"
        state: present
        sysctl_set: yes
      loop:
        - { name: "net.ipv4.tcp_keepalive_time", value: "120" }
        - { name: "net.ipv4.ip_forward", value: "1" }
        - { name: "net.bridge.bridge-nf-call-iptables", value: "1" }
        - { name: "net.bridge.bridge-nf-call-ip6tables", value: "1" }

    - name: Create necessary directories
      file:
        path: "{{ item }}"
        state: directory
        mode: "0755"
      loop:
        - /etc/consul
        - /etc/nomad
        - /opt/consul
        - /opt/nomad
        - /var/lib/consul
        - /var/lib/nomad

    - name: Add HashiCorp GPG key
      apt_key:
        url: https://apt.releases.hashicorp.com/gpg
        state: present

    - name: Add HashiCorp repository
      apt_repository:
        repo: "deb [arch=amd64] https://apt.releases.hashicorp.com {{ ansible_distribution_release }} main"
        state: present

    - name: Check if systemd-resolved is available
      stat:
        path: /etc/systemd/resolved.conf
      register: systemd_resolved_check

    - name: Configure systemd-resolved for Consul DNS
      blockinfile:
        path: /etc/systemd/resolved.conf
        block: |
          [Resolve]
          DNS=127.0.0.1
          Domains=~consul
          DNSStubListener=no
      when: systemd_resolved_check.stat.exists
      notify: restart systemd-resolved

    - name: Configure /etc/resolv.conf for Consul DNS (fallback)
      lineinfile:
        path: /etc/resolv.conf
        line: "nameserver 127.0.0.1"
        insertbefore: BOF
      when: not systemd_resolved_check.stat.exists

    - name: Final base setup summary
      debug:
        msg: |
          ========================================
          Base Setup Complete for {{ inventory_hostname }}
          ========================================
          - Base packages installed
          - Timezone set to UTC
          - Sysctl settings configured
          - HashiCorp repository added
          - Consul/Nomad directories created
          - DNS configuration prepared for Consul

          Container is ready for service installation:
          - Run 02-install-consul.yml to install Consul
          - Run 03-install-nomad.yml to install Nomad
          - Run 05-install-postgres.yml for database nodes
          ========================================

# Verification checks at the end
- name: Verify Infrastructure Setup
  hosts: proxmox_nodes
  gather_facts: false
  tasks:
    - name: Run infrastructure verification checks
      block:
        - name: Check all containers are running
          shell: |
            failed=0
            for vmid in {% for container in hostvars[inventory_hostname]['my_containers'] %}{{ container.id }} {% endfor %}; do
              status=$(pct status $vmid 2>/dev/null | grep -oP 'status: \K\w+' || echo "not_found")
              if [ "$status" != "running" ]; then
                echo "Container $vmid is not running (status: $status)"
                failed=1
              fi
            done
            exit $failed
          register: container_check
          changed_when: false
          failed_when: container_check.rc != 0

        - name: Verify Tailscale connectivity
          shell: |
            failed=0
            {% for container in hostvars[inventory_hostname]['my_containers'] %}
            # Check if tailscale is running and get IP
            ip=$(pct exec {{ container.id }} -- bash -c "tailscale ip -4 2>/dev/null || echo ''" 2>/dev/null | tr -d '\n')
            if [ -z "$ip" ]; then
              echo "Container {{ container.id }} ({{ container.name }}) - No Tailscale IP assigned"
              failed=1
            else
              echo "Container {{ container.id }} ({{ container.name }}) - Tailscale IP: $ip"
            fi
            {% endfor %}
            exit $failed
          register: tailscale_check
          changed_when: false
          failed_when: tailscale_check.rc != 0
          retries: 3
          delay: 20
          until: tailscale_check.rc == 0

        - name: Verify SSH connectivity to containers
          shell: |
            failed=0
            {% for container in hostvars[inventory_hostname]['my_containers'] %}
            {% set container_name = container.name %}
            {% for host in groups['all'] %}
            {% if hostvars[host]['inventory_hostname'] == container_name %}
            if ! ssh -o ConnectTimeout=5 -o StrictHostKeyChecking=no {{ hostvars[host].tailscale_ip | default('') }} "echo 'SSH OK'" &>/dev/null; then
              echo "Cannot SSH to {{ container_name }} ({{ hostvars[host].tailscale_ip | default('no IP') }})"
              failed=1
            fi
            {% endif %}
            {% endfor %}
            {% endfor %}
            exit $failed
          register: ssh_check
          changed_when: false
          failed_when: false # Don't fail, just report

        - name: Display verification summary
          debug:
            msg: |
              ========================================
              Infrastructure Verification Summary
              ========================================
              Container Status: {{ 'PASS' if container_check.rc == 0 else 'FAIL' }}
              Tailscale Status: {{ 'PASS' if tailscale_check.rc == 0 else 'FAIL' }}
              SSH Connectivity: {{ 'PASS' if ssh_check.rc == 0 else 'WARNING' }}

              {% if container_check.stdout %}
              Container Issues:
              {{ container_check.stdout }}
              {% endif %}

              {% if tailscale_check.stdout %}
              Tailscale Info:
              {{ tailscale_check.stdout }}
              {% endif %}

              {% if ssh_check.stdout %}
              SSH Issues:
              {{ ssh_check.stdout }}
              {% endif %}
              ========================================

  handlers:
    - name: restart systemd-resolved
      systemd:
        name: systemd-resolved
        state: restarted
